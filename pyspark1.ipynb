{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxVJlevX7NuU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oujpGAhs7OXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_date_spark = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .csv(\"data/dim_Date.csv\")\n",
        "\n",
        "df_attendancesessions_spark = spark.read.parquet(\"data/fact_AttendanceSession\")\n",
        "df_organisation_spark = spark.read.parquet(\"data/dim_Organisation\")\n",
        "df_student_spark = spark.read.parquet(\"data/dim_Student\")\n",
        "df_studentextended_spark = spark.read.parquet(\"data/dim_StudentExtended\")"
      ],
      "metadata": {
        "id": "BafP3CG27O2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import countDistinct\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "af7GL_so7ZdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_df_missing_breakdown(df: DataFrame) -> None:\n",
        "\n",
        "    total_rows = df.count()\n",
        "    total_cols = len(df.columns)\n",
        "\n",
        "    agg_exprs = []\n",
        "    for field in df.schema.fields:\n",
        "        col_name = field.name\n",
        "        is_numeric = field.dataType.typeName() in (\n",
        "            \"double\", \"float\", \"decimal\",\n",
        "            \"integer\", \"long\", \"short\", \"byte\"\n",
        "        )\n",
        "\n",
        "        # Count NULLs\n",
        "        null_count_expr = F.sum(\n",
        "            F.when(F.col(col_name).isNull(), 1).otherwise(0)\n",
        "        ).alias(col_name + \"_nullCount\")\n",
        "\n",
        "\n",
        "        empty_count_expr = F.sum(\n",
        "            F.when(F.col(col_name).cast(\"string\") == \"\", 1).otherwise(0)\n",
        "        ).alias(col_name + \"_emptyCount\")\n",
        "\n",
        "        na_str_expr = F.sum(\n",
        "            F.when(\n",
        "                F.upper(F.col(col_name).cast(\"string\")).isin(\"NA\", \"NAN\"),\n",
        "                1\n",
        "            ).otherwise(0)\n",
        "        ).alias(col_name + \"_naStrCount\")\n",
        "\n",
        "        # Count numeric NaN (only for numeric columns)\n",
        "        if is_numeric:\n",
        "            nan_numeric_expr = F.sum(\n",
        "                F.when(F.isnan(F.col(col_name)), 1).otherwise(0)\n",
        "            ).alias(col_name + \"_nanNumericCount\")\n",
        "        else:\n",
        "            # For non-numeric columns, this will always be 0\n",
        "            nan_numeric_expr = F.lit(0).alias(col_name + \"_nanNumericCount\")\n",
        "\n",
        "        # Collect all expressions\n",
        "        agg_exprs.extend([\n",
        "            null_count_expr, empty_count_expr, na_str_expr, nan_numeric_expr\n",
        "        ])\n",
        "\n",
        "    # Perform a single pass to get all missing counts\n",
        "    agg_df = df.select(agg_exprs)\n",
        "    result_row = agg_df.collect()[0].asDict()  # single row with all counts\n",
        "\n",
        "    # Print header\n",
        "    print(f\"DataFrame has {total_rows} rows and {total_cols} columns.\\n\")\n",
        "    print(\n",
        "        \"Column                             \"\n",
        "        \"Null  EmptyStr  NA/NaNStr  NumericNaN  TotalMissing  %Missing\"\n",
        "    )\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Loop over columns and print breakdown\n",
        "    for field in df.schema.fields:\n",
        "        c = field.name\n",
        "        null_count = result_row[c + \"_nullCount\"]\n",
        "        empty_count = result_row[c + \"_emptyCount\"]\n",
        "        na_str_count = result_row[c + \"_naStrCount\"]\n",
        "        nan_numeric_count = result_row[c + \"_nanNumericCount\"]\n",
        "\n",
        "        total_missing = null_count + empty_count + na_str_count + nan_numeric_count\n",
        "        pct_missing = (total_missing / total_rows * 100) if total_rows else 0.0\n",
        "\n",
        "        print(\n",
        "            f\"{c:34s}\"\n",
        "            f\"{null_count:5d}\"\n",
        "            f\"{empty_count:10d}\"\n",
        "            f\"{na_str_count:10d}\"\n",
        "            f\"{nan_numeric_count:12d}\"\n",
        "            f\"{total_missing:13d}\"\n",
        "            f\"{pct_missing:10.2f}%\"\n",
        "        )"
      ],
      "metadata": {
        "id": "ZB4O413O7be8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef show_distinct_counts(df: DataFrame, top_n: int = 20) -> None:\n",
        "    \"\"\"\n",
        "    Displays the number of distinct values in each column of the DataFrame\n",
        "    and lists the top_n columns with the highest distinct counts.\n",
        "\n",
        "    Additionally, creates and displays a DataFrame containing all columns with their distinct counts.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The Spark DataFrame to analyze.\n",
        "    top_n (int): The number of top columns to display based on distinct counts.\n",
        "    \"\"\"\n",
        "    # Calculate distinct counts for each column\n",
        "    distinct_counts = df.agg(*[countDistinct(c).alias(c) for c in df.columns]).collect()[0].asDict()\n",
        "\n",
        "    # Sort columns by distinct count in descending order\n",
        "    sorted_counts = sorted(distinct_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Display the top_n columns\n",
        "    print(f\"{'Column':34s} {'Distinct Count'}\")\n",
        "    print(\"-\" * 50)\n",
        "    for col, cnt in sorted_counts[:top_n]:\n",
        "        print(f\"{col:34s} {cnt}\")\n",
        "\n",
        "    # Create a DataFrame of all distinct counts\n",
        "    df_distinct_counts = spark.createDataFrame(sorted_counts, [\"Column\", \"Distinct_Count\"])\n",
        "\n",
        "    # Show the DataFrame of distinct counts\n",
        "    print(\"\\nAll Column Distinct Counts:\")"
      ],
      "metadata": {
        "id": "8y8te9CY70QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
        "\n",
        "def show_distinct_counts_approx(df: DataFrame, top_n: int = 20, rsd: float = 0.05) -> None:\n",
        "    \"\"\"\n",
        "    Displays the approximate number of distinct values in each column of the DataFrame\n",
        "    and lists the top_n columns with the highest distinct counts.\n",
        "\n",
        "    Additionally, creates and displays a DataFrame containing all columns with\n",
        "    their approximate distinct counts, but only shows the top_n rows to reduce\n",
        "    the chance of memory/network issues.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        The Spark DataFrame to analyze.\n",
        "    top_n : int\n",
        "        The number of top columns to display based on distinct counts.\n",
        "    rsd : float\n",
        "        Relative Standard Deviation for approx_count_distinct.\n",
        "        Lower = more accurate but more memory usage. Typical default is 0.05.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a list of approx_count_distinct expressions for each column\n",
        "    approx_exprs = [\n",
        "        F.approx_count_distinct(F.col(c), rsd=rsd).alias(c)\n",
        "        for c in df.columns\n",
        "    ]\n",
        "\n",
        "    # Collect the single row of approximate distinct counts as a dict\n",
        "    #  e.g. {'colA': 123, 'colB': 999, ...}\n",
        "    approx_counts_row = df.agg(*approx_exprs).collect()[0].asDict()\n",
        "\n",
        "    # Convert that dict into a list of (column, distinct_count) tuples and sort\n",
        "    sorted_counts = sorted(approx_counts_row.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print header\n",
        "    print(f\"{'Column':34s} {'Approx Distinct Count'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Show only the top_n columns in console\n",
        "    for col_name, cnt in sorted_counts[:top_n]:\n",
        "        print(f\"{col_name:34s} {cnt}\")\n",
        "\n",
        "    # Create a small DataFrame from the sorted counts\n",
        "    # Each row: (column_name, approx_distinct_count)\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df_approx_counts = spark.createDataFrame(\n",
        "        sorted_counts, [\"Column\", \"ApproxDistinctCount\"]\n",
        "    )\n",
        "\n",
        "    # Show only the top_n rows, so we don't blow up memory\n",
        "    print(\"\\nAll Column Approx Distinct Counts (showing top_n only):\")\n",
        "    df_approx_counts.limit(top_n).show(truncate=False)"
      ],
      "metadata": {
        "id": "LP3z30aK74_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_studentextended_spark)"
      ],
      "metadata": {
        "id": "8WKYNCdb77iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_student_spark.show()"
      ],
      "metadata": {
        "id": "X5hnLHxH79VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_student_spark)"
      ],
      "metadata": {
        "id": "ZNO5tG0o7_RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_organisation_spark.show()"
      ],
      "metadata": {
        "id": "Y8wWWXLO8BWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_organisation_spark)\n"
      ],
      "metadata": {
        "id": "58DqDXEM8Dnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_attendancesessions_spark.show()\n"
      ],
      "metadata": {
        "id": "3rsdk5LO8FOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_attendancesessions_spark)\n"
      ],
      "metadata": {
        "id": "AbppCAbQ8G3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Creat a new column datekey in the df_attendancesessions_spark which will take the value of the Date column without the \"-\" character.\n",
        "#This can act as key to join the df_attendancesessions_spark with the df_date_spark\n",
        "\n",
        "df_attendancesessions_spark = df_attendancesessions_spark.withColumn(\n",
        "    \"datekey\",\n",
        "    F.regexp_replace(\"Date\", \"-\", \"\").cast(\"int\")\n",
        ")\n",
        "df_attendancesessions_spark.show()"
      ],
      "metadata": {
        "id": "GpokV_RR8JQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_date_spark.show() #check the df_date_spark\n"
      ],
      "metadata": {
        "id": "x0erxbz58K27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_date_spark)"
      ],
      "metadata": {
        "id": "r3oq2qUX8dET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Alias DataFrames to reference them in the join condition and in the column selection\n",
        "df_att_aliased = df_attendancesessions_spark.alias(\"att\")\n",
        "df_org_aliased = df_organisation_spark.alias(\"org\")\n",
        "df_stu_aliased = df_student_spark.alias(\"stu\")\n",
        "df_stex_aliased = df_studentextended_spark.alias(\"stex\")\n",
        "df_date_aliased = df_date_spark.alias(\"dd\")\n",
        "\n",
        "# 2. Join them explicitly\n",
        "df_joined = (\n",
        "    df_att_aliased\n",
        "    .join(df_org_aliased, df_att_aliased[\"organisationkey\"] == df_org_aliased[\"organisationkey\"], \"left\")\n",
        "    .join(df_stu_aliased, df_att_aliased[\"studentkey\"] == df_stu_aliased[\"studentkey\"], \"left\")\n",
        "    .join(df_stex_aliased, df_att_aliased[\"studentkey\"] == df_stex_aliased[\"studentkey\"], \"left\")\n",
        "    .join(df_date_aliased, df_att_aliased[\"datekey\"] == df_date_aliased[\"DateKey\"], \"left\") #join the df_attendancesessions_spark with the df_date_spark\n",
        ")\n",
        "\n",
        "# 3. Programmatically build a list of columns to select\n",
        "#    Each column is referenced by alias + column name, and renamed with a prefix\n",
        "att_cols = [F.col(f\"att.{c}\").alias(f\"att_{c}\") for c in df_attendancesessions_spark.columns]\n",
        "org_cols = [F.col(f\"org.{c}\").alias(f\"org_{c}\") for c in df_organisation_spark.columns]\n",
        "stu_cols = [F.col(f\"stu.{c}\").alias(f\"stu_{c}\") for c in df_student_spark.columns]\n",
        "stex_cols = [F.col(f\"stex.{c}\").alias(f\"stex_{c}\") for c in df_studentextended_spark.columns]\n",
        "date_cols = [F.col(f\"dd.{c}\").alias(f\"dd_{c}\") for c in df_date_spark.columns]\n",
        "\n",
        "# Combine all these column lists\n",
        "all_cols = att_cols + org_cols + stu_cols + stex_cols + date_cols\n",
        "\n",
        "# 4. Select everything into a new DataFrame, with prefixed column names\n",
        "df_joined_renamed = df_joined.select(*all_cols)\n",
        "\n",
        "df_joined_renamed.show(truncate=False)"
      ],
      "metadata": {
        "id": "CMF46ypN8f51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the data types of the columns\n",
        "df_joined_renamed.dtypes"
      ],
      "metadata": {
        "id": "d9oceiF_8iA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = df_joined_renamed.count()\n",
        "print(f\"Total rows: {total_rows}\")"
      ],
      "metadata": {
        "id": "LyMobMiZ8kqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_count = len(df_joined_renamed.columns)\n",
        "print(f\"Number of columns: {column_count}\")"
      ],
      "metadata": {
        "id": "UjM_9Cz-8nGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_selected = (\n",
        "    df_joined_renamed\n",
        "    .select(\n",
        "        # --- Student details & school info ---\n",
        "        F.col(\"stu_Sex\").alias(\"gender\"),\n",
        "        F.col(\"stu_Forename\").alias(\"student_forename\"),\n",
        "        F.col(\"stu_Surname\").alias(\"student_surname\"),\n",
        "        F.col(\"stex_Pupil_Premium_Indicator\").alias(\"pupil_premium\"),\n",
        "        F.col(\"stex_Year_Group\").alias(\"year_group\"),\n",
        "        F.col(\"stex_Current_NC_Year\").alias(\"nc_year\"),\n",
        "        F.col(\"org_Organisation_Type\").alias(\"school_type\"),\n",
        "        F.col(\"org_Organisation_Name\").alias(\"school\"),\n",
        "        F.col(\"org_Establishment_Number\").alias(\"establishment_number\"),\n",
        "        F.col(\"org_LA_Code\").alias(\"la_code\"),\n",
        "\n",
        "        # --- Dates ---\n",
        "        F.col(\"att_Date\").alias(\"attendance_date\"),\n",
        "        F.col(\"dd_AcademicYear\").alias(\"academic_year\"),\n",
        "        F.col(\"dd_AcademicWeekNumberOfYear\").alias(\"academic_week_number\"),\n",
        "        F.col(\"dd_TermSession\").alias(\"term\"),\n",
        "        # Replace below if \"weekcommencingdate\" doesn't exist.\n",
        "        # For example, use \"dd_WeekStartDate\" or \"dd_WeekCommencing DD/MM/YYYY\" from your schema.\n",
        "        F.col(\"dd_WeekCommencingName\").alias(\"weekcommencing\"),\n",
        "\n",
        "        # --- Attendance fields ---\n",
        "        F.col(\"att_Mark\").alias(\"mark\"),\n",
        "        F.col(\"att_Session\").alias(\"session\"),\n",
        "        # Use a valid alias for 'att_is_aea' (spaces in column names can cause issues)\n",
        "        F.col(\"att_is_aea\").alias(\"is_approved_educational_activity\"),\n",
        "        F.col(\"att_is_attend\").alias(\"is_attend\"),\n",
        "        F.col(\"att_is_auth_abs\").alias(\"is_auth_abs\"),\n",
        "        F.col(\"att_is_late_L\").alias(\"late\"),\n",
        "        F.col(\"att_is_late_U\").alias(\"late_unauthorised\"),\n",
        "        F.col(\"att_is_missing\").alias(\"missing\"),\n",
        "        F.col(\"att_is_nr\").alias(\"no_reason\"),\n",
        "        F.col(\"att_is_possible\").alias(\"is_possible\"),\n",
        "        F.col(\"att_is_present\").alias(\"is_present\"),\n",
        "        F.col(\"att_is_unauth_abs\").alias(\"is_unauth_abs\"),\n",
        "\n",
        "        # --- Current student info ---\n",
        "        F.col(\"stex_Is_Current\").alias(\"current_student\"),\n",
        "        F.col(\"stex_Leaving_Date\").alias(\"leaving_date\"),\n",
        "        F.col(\"stu_UPN\").alias(\"UPN\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_selected.show(truncate=False)"
      ],
      "metadata": {
        "id": "uVBwrpvb8qnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected.select(\"year_group\").distinct().show()"
      ],
      "metadata": {
        "id": "KaM-ANtp8sw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tidy = (\n",
        "    df_selected\n",
        "    .withColumn(\n",
        "        \"year_group_tidy\",\n",
        "        F.when(\n",
        "            F.col(\"year_group\").isin(\"Nursery 1\", \"Nursery 2\"),\n",
        "            F.regexp_replace(\"year_group\", \"Nursery \", \"N\")\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"year_group\") == \"R\",\n",
        "            F.lit(\"Reception\")\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"year_group\") == \"Year 13\",\n",
        "            F.lit(\"Y13\")\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"year_group\").rlike(\"^[0-9]+$\"),\n",
        "            F.concat(F.lit(\"Y\"), F.col(\"year_group\").cast(\"int\"))\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"year_group\").rlike(\"^Y[0-9]{1,2}$\"),\n",
        "            F.concat(F.lit(\"Y\"), F.regexp_replace(\"year_group\", \"^[Yy]\", \"\").cast(\"int\"))\n",
        "        )\n",
        "        .otherwise(F.col(\"year_group\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get distinct values\n",
        "distinct_vals = df_tidy.select(\"year_group_tidy\").distinct()\n",
        "count_distinct = distinct_vals.count()\n",
        "\n",
        "print(f\"Number of distinct values in 'year_group_tidy': {count_distinct}\")\n",
        "distinct_vals.show(truncate=False)"
      ],
      "metadata": {
        "id": "7x-_JsHm8tOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_tidy.select(\"year_group_tidy\"))"
      ],
      "metadata": {
        "id": "7v8hqIMP8vHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tidy.groupBy(\"year_group_tidy\").count().show()"
      ],
      "metadata": {
        "id": "-A2x_S-u8w3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected.select(\"nc_year\").distinct().show()"
      ],
      "metadata": {
        "id": "6o-bO_wY8yp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of students in each NC year\n",
        "df_selected.groupBy(\"nc_year\").count().show()"
      ],
      "metadata": {
        "id": "ueqr1my581kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_missing_breakdown(df_selected.select(\"nc_year\"))\n"
      ],
      "metadata": {
        "id": "naV_wOyJ82DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of distinct values in the columns all except 'student_forename','student_surname', and 'UPN'\n",
        "\n",
        "show_distinct_counts_approx(df_selected.drop(\"student_forename\", \"attendance_date\", \"student_surname\", \"UPN\"))\n"
      ],
      "metadata": {
        "id": "4BPW50pi83mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list the distinct values in the column 'mark'\n",
        "df_selected.select(\"mark\").distinct().show()\n",
        "df_selected.select(\"academic_year\").distinct().show()\n"
      ],
      "metadata": {
        "id": "oepCGco785ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "OWYpLWCF878m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a new field by concatenating UPN and attendance_date and session (AM/PM) with an underscore separator\n",
        "#this will be used to identify the unique attendance record for each student per day per session\n",
        "df_with_combined = df_selected.withColumn(\n",
        "    \"UPN_AttendanceDate\",\n",
        "    F.concat_ws(\"_\", F.col(\"UPN\"), F.col(\"attendance_date\"), F.col(\"session\"))\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Group by this new field and filter for count == 1 (i.e. unique)\n",
        "df_valid = (\n",
        "    df_with_combined\n",
        "    .groupBy(\"UPN_AttendanceDate\")\n",
        "    .count()\n",
        "    .filter(F.col(\"count\") == 1) #each student can have only one attendance per day per session AM or PM\n",
        ")\n",
        "\n",
        "# 3. Group by this new field and filter for count > 1 (i.e. duplicates)\n",
        "df_invalid = (\n",
        "    df_with_combined\n",
        "    .groupBy(\"UPN_AttendanceDate\")\n",
        "    .count()\n",
        "    .filter(F.col(\"count\") > 1)\n",
        ")\n",
        "\n",
        "\n",
        "df_with_combined.show(truncate=False)"
      ],
      "metadata": {
        "id": "SZ3H3nsG89g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_invalid.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "3kovJhvW9ADF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total rows with original df: {df_joined_renamed.count()}\")\n",
        "\n",
        "print(f\"Total rows with selected data df: {df_selected.count()}\")\n",
        "\n",
        "print(f\"Total rows valid data: {df_valid.count()}\")\n",
        "\n",
        "print(f\"Total rows invalid data: {df_invalid.count()}\")\n"
      ],
      "metadata": {
        "id": "WuDfk39k9BtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Aggregate all keys with their counts\n",
        "df_counts = (\n",
        "    df_with_combined\n",
        "    .groupBy(\"UPN_AttendanceDate\")\n",
        "    .agg(F.count(\"*\").alias(\"count\"))\n",
        "    .withColumn(\n",
        "        \"status\",\n",
        "        F.when(F.col(\"count\") == 1, \"valid\").otherwise(\"invalid\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# 2. Join back to original rows to get the full data plus the status\n",
        "df_with_status = (\n",
        "    df_with_combined.alias(\"a\")\n",
        "    .join(df_counts.alias(\"b\"), on=\"UPN_AttendanceDate\", how=\"left\")\n",
        "    .select(\"a.*\", \"b.count\", \"b.status\")\n",
        ")\n",
        "\n",
        "df_with_status.show(truncate=False)"
      ],
      "metadata": {
        "id": "pygM2VNu9EFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary = (\n",
        "    df_validf_rows\n",
        "    .groupBy(\"school\",  \"nc_year\", \"weekcommencing\")\n",
        "    .agg(\n",
        "        F.round(\n",
        "            (F.sum(\"is_attend\") / F.sum(\"is_possible\") * 100), 1\n",
        "        ).alias(\"attendance_percentage\")\n",
        "    )\n",
        ")\n",
        "\n",
        "#df_summary.limit(20).show(truncate=False)\n",
        "df_summary.show(truncate=False)"
      ],
      "metadata": {
        "id": "bbp9Ml7H9F7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary.limit(20).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "nbLylkIE9JIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df_summary.columns\n",
        "\n",
        "# Build a filter condition: (col1 IS NULL) OR (col2 IS NULL) OR ...\n",
        "null_condition = reduce(lambda acc, c: acc | F.col(c).isNull(), cols, F.lit(False))\n",
        "\n",
        "# Filter rows where any column is null\n",
        "num_rows_with_null = df_summary.filter(null_condition).count()\n",
        "\n",
        "print(f\"Number of rows with at least one NULL value: {num_rows_with_null}\")"
      ],
      "metadata": {
        "id": "8TlP7pjG9K_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) From df_selected, group by the same columns used in df_summary\n",
        "df_sums = (\n",
        "    df_validf_rows\n",
        "        .groupBy(\"school\",  \"nc_year\", \"weekcommencing\")\n",
        "        .agg(\n",
        "            F.sum(\"is_attend\").alias(\"sum_is_attend\"),\n",
        "            F.sum(\"is_possible\").alias(\"sum_is_possible\")\n",
        "        )\n",
        ")\n",
        "\n",
        "\n",
        "# 2) Filter df_summary for rows where attendance_percentage is NULL\n",
        "df_summary_nulls = df_summary.filter(F.col(\"attendance_percentage\").isNull())\n",
        "\n",
        "# 3) Join df_summary_nulls with df_sums to see actual sums for those groups\n",
        "df_null_sums = (\n",
        "    df_summary_nulls.alias(\"summ\")\n",
        "    .join(\n",
        "        df_sums.alias(\"sums\"),\n",
        "        on=[\"school\", \"nc_year\", \"weekcommencing\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .select(\n",
        "        \"summ.school\",\n",
        "        \"summ.nc_year\",\n",
        "        \"summ.weekcommencing\",\n",
        "        \"summ.attendance_percentage\",   # should be NULL\n",
        "        \"sums.sum_is_attend\",\n",
        "        \"sums.sum_is_possible\"\n",
        "    )\n",
        ")\n",
        "\n",
        "df_null_sums.show(truncate=False)"
      ],
      "metadata": {
        "id": "5T_rk1oj9M_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validf_rows.filter(\n",
        "  (F.col(\"school\") == \"Academy 7\") &\n",
        "  (F.col(\"nc_year\") == \"9\") &\n",
        "  (F.col(\"weekcommencing\") == \"w/c 21/08/2023\")\n",
        "\n",
        ").show()"
      ],
      "metadata": {
        "id": "G6sTHKbf9Ph_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary = (\n",
        "    df_invalidf_rows\n",
        "\n",
        "    # 1) Group and aggregate\n",
        "    .groupBy(\"school\", \"nc_year\", \"weekcommencing\")\n",
        "    .agg(\n",
        "        F.round(\n",
        "            (F.sum(\"is_attend\") / F.sum(\"is_possible\") * 100), 1\n",
        "        ).alias(\"attendance_percentage\")\n",
        "    )\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "df_summary.limit(20).show(truncate=False)"
      ],
      "metadata": {
        "id": "TDINcL3d9Rjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Basic statistics for the attendance_percentage column\n",
        "\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "df_summary.describe(\"attendance_percentage\") \\\n",
        "    .select(\"summary\", round(\"attendance_percentage\", 1).alias(\"attendance_percentage\")) \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "r20gCRG79THq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conver df_summary to pandas\n",
        "df_summary_pandas = df_summary.toPandas()\n",
        "\n",
        "#convert pandas to parquet\n",
        "df_summary_pandas.to_parquet('data/df_summary_pandas.parquet')\n",
        "#chceck if the file is created\n",
        "df_summary_loaded = spark.read.parquet(\"data/df_summary_pandas.parquet\")\n",
        "df_summary_loaded.show(truncate=False)"
      ],
      "metadata": {
        "id": "r8UebhEd9Vgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqzIEMMU9YG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}