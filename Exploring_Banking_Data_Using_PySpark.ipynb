{
  "cells": [
    {
      "source": [
        "\n",
        "import kagglehub\n",
        "rouseguy_bankbalanced_path = kagglehub.dataset_download('rouseguy/bankbalanced')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "yRLuhIyd4RdG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PTqZxz2o4RdI"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "IITATVZh4RdI"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rUEAu0RV4RdJ"
      },
      "cell_type": "code",
      "source": [
        "#!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "c5O_l3bY4RdK"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
        "df = spark.read.csv('../input/bank.csv', header = True, inferSchema = True)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mh5QAdol4RdK"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IKlKbSL4RdK"
      },
      "cell_type": "markdown",
      "source": []
    },
    {
      "metadata": {
        "id": "v5ZIyiMz4RdK"
      },
      "cell_type": "markdown",
      "source": [
        "**Analyzing Data**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3okCtFBD4RdK"
      },
      "cell_type": "code",
      "source": [
        "df.groupby('deposit').count().toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "B_M6x3eM4RdK"
      },
      "cell_type": "code",
      "source": [
        "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
        "df.select(numeric_features).describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "to69zDCV4RdL"
      },
      "cell_type": "code",
      "source": [
        "numeric_data = df.select(numeric_features).toPandas()\n",
        "axs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\n",
        "n = len(numeric_data.columns)\n",
        "for i in range(n):\n",
        "    v = axs[i, 0]\n",
        "    v.yaxis.label.set_rotation(0)\n",
        "    v.yaxis.label.set_ha('right')\n",
        "    v.set_yticks(())\n",
        "    h = axs[n-1, i]\n",
        "    h.xaxis.label.set_rotation(90)\n",
        "    h.set_xticks(())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qCNGkSqk4RdL"
      },
      "cell_type": "code",
      "source": [
        "df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
        "cols = df.columns\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MGHDm2Rt4RdL"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
        "\n",
        "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "\n",
        "stages = []\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "\n",
        "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
        "\n",
        "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "\n",
        "    stages += [stringIndexer, encoder]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xYGp1hv-4RdM"
      },
      "cell_type": "markdown",
      "source": [
        "We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kKIfCUa24RdM"
      },
      "cell_type": "code",
      "source": [
        "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
        "\n",
        "stages += [label_stringIdx]\n",
        "\n",
        "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
        "\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "\n",
        "stages += [assembler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xHo5I7Dx4RdM"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = stages)\n",
        "pipelineModel = pipeline.fit(df)\n",
        "df = pipelineModel.transform(df)\n",
        "selectedCols = ['label', 'features'] + cols\n",
        "df = df.select(selectedCols)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EXgZ_r1p4RdM"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q-RGe1FH4RdM"
      },
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzrERZvm4RdM"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating Logistic Regression Model**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QgohLizA4RdN"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
        "lrModel = lr.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "U2amLJuk4RdN"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "beta = np.sort(lrModel.coefficients)\n",
        "plt.plot(beta)\n",
        "plt.ylabel('Beta Coefficients')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3woc0Uu24RdN"
      },
      "cell_type": "code",
      "source": [
        "trainingSummary = lrModel.summary\n",
        "roc = trainingSummary.roc.toPandas()\n",
        "plt.plot(roc['FPR'],roc['TPR'])\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()\n",
        "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdj0PjGM4RdN"
      },
      "cell_type": "markdown",
      "source": [
        "**Precision and Recall**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "I3_1oSN54RdN"
      },
      "cell_type": "code",
      "source": [
        "pr = trainingSummary.pr.toPandas()\n",
        "plt.plot(pr['recall'],pr['precision'])\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Recall')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lD-AK5sx4RdN"
      },
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(test)\n",
        "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQjndAp_4RdN"
      },
      "cell_type": "markdown",
      "source": [
        "**Evaluate Logistic Regression**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LLxRsYze4RdN"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "print('Test Area Under ROC', evaluator.evaluate(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3Pi_dqoe4RdO"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
        "dtModel = dt.fit(train)\n",
        "predictions = dtModel.transform(test)\n",
        "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GGvI3R-s4RdO"
      },
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJ7e97y84RdO"
      },
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Cl_eQ6dE4RdO"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
        "rfModel = rf.fit(train)\n",
        "predictions = rfModel.transform(test)\n",
        "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "491MqtbO4RdO"
      },
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cBm0giF4RdO"
      },
      "cell_type": "markdown",
      "source": [
        "**Gradient-Boosted Tree Classifier**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-M7_Sr-i4RdO"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(maxIter=10)\n",
        "gbtModel = gbt.fit(train)\n",
        "predictions = gbtModel.transform(test)\n",
        "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "w-6xteDK4RdP"
      },
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3omoR2h34RdP"
      },
      "cell_type": "code",
      "source": [
        "print(gbt.explainParams())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "06rBS6F34RdP"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
        "             .addGrid(gbt.maxBins, [20, 60])\n",
        "             .addGrid(gbt.maxIter, [10, 20])\n",
        "             .build())\n",
        "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
        "cvModel = cv.fit(train)\n",
        "predictions = cvModel.transform(test)\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Exploring Banking Data Using PySpark",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}